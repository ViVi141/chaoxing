# -*- coding: utf-8 -*-
"""
æ•°æ®åº“è¿ç§»å·¥å…· - SQLiteè¿ç§»åˆ°PostgreSQL
æ”¯æŒWebç•Œé¢è°ƒç”¨ï¼Œå¸¦è¿›åº¦å›è°ƒ
"""
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent.parent))

import json
import asyncio
from typing import Optional, Callable, Dict, Any
from datetime import datetime
from sqlalchemy import create_engine, text, MetaData
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession, async_sessionmaker
from sqlalchemy.orm import sessionmaker
import aiosqlite

from api.logger import logger
from models import (
    User,
    UserConfig,
    Task,
    TaskLog,
    SystemLog,
    EmailVerification,
    SystemConfig,
)


class DatabaseMigrationError(Exception):
    """è¿ç§»é”™è¯¯"""

    pass


class DatabaseMigrator:
    """æ•°æ®åº“è¿ç§»å™¨"""

    def __init__(
        self,
        source_db_url: str,
        target_db_url: str,
        progress_callback: Optional[Callable] = None,
    ):
        """
        åˆå§‹åŒ–è¿ç§»å™¨

        Args:
            source_db_url: æºæ•°æ®åº“URLï¼ˆSQLiteï¼‰
            target_db_url: ç›®æ ‡æ•°æ®åº“URLï¼ˆPostgreSQLï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•° callback(step, progress, message)
        """
        self.source_db_url = source_db_url
        self.target_db_url = target_db_url
        self.progress_callback = progress_callback

        # è¡¨è¿ç§»é¡ºåºï¼ˆè€ƒè™‘å¤–é”®ä¾èµ–ï¼‰
        self.table_order = [
            "users",
            "user_configs",
            "tasks",
            "task_logs",
            "system_logs",
            "email_verifications",
            "system_configs",
        ]

        self.models_map = {
            "users": User,
            "user_configs": UserConfig,
            "tasks": Task,
            "task_logs": TaskLog,
            "system_logs": SystemLog,
            "email_verifications": EmailVerification,
            "system_configs": SystemConfig,
        }

    def _report_progress(self, step: str, progress: int, message: str):
        """æŠ¥å‘Šè¿›åº¦"""
        logger.info(f"[è¿ç§»è¿›åº¦ {progress}%] {step}: {message}")
        if self.progress_callback:
            try:
                self.progress_callback(step, progress, message)
            except Exception as e:
                logger.warning(f"è¿›åº¦å›è°ƒå¤±è´¥: {e}")

    async def test_target_connection(self) -> bool:
        """æµ‹è¯•ç›®æ ‡æ•°æ®åº“è¿æ¥"""
        try:
            engine = create_async_engine(self.target_db_url, echo=False)
            async with engine.connect() as conn:
                await conn.execute(text("SELECT 1"))
            await engine.dispose()
            logger.info("âœ… ç›®æ ‡æ•°æ®åº“è¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ ç›®æ ‡æ•°æ®åº“è¿æ¥æµ‹è¯•å¤±è´¥: {e}")
            raise DatabaseMigrationError(f"æ— æ³•è¿æ¥åˆ°ç›®æ ‡æ•°æ®åº“: {str(e)}")

    async def backup_source_database(self, backup_path: str) -> str:
        """å¤‡ä»½æºæ•°æ®åº“"""
        try:
            self._report_progress("å¤‡ä»½", 5, "æ­£åœ¨å¤‡ä»½SQLiteæ•°æ®åº“...")

            # æå–SQLiteæ–‡ä»¶è·¯å¾„
            if ":///" in self.source_db_url:
                db_file = self.source_db_url.split(":///")[-1]
                # å¤„ç†ç›¸å¯¹è·¯å¾„
                if db_file.startswith("./"):
                    db_file = db_file[2:]

                source_path = Path(db_file)
                if not source_path.exists():
                    raise DatabaseMigrationError(f"æºæ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {source_path}")

                # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_file = Path(backup_path) / f"chaoxing_backup_{timestamp}.db"
                backup_file.parent.mkdir(parents=True, exist_ok=True)

                # å¤åˆ¶æ–‡ä»¶
                import shutil

                shutil.copy2(source_path, backup_file)

                self._report_progress("å¤‡ä»½", 10, f"å¤‡ä»½å®Œæˆ: {backup_file}")
                logger.info(f"âœ… æ•°æ®åº“å¤‡ä»½æˆåŠŸ: {backup_file}")
                return str(backup_file)
            else:
                raise DatabaseMigrationError("ä¸æ”¯æŒçš„æ•°æ®åº“URLæ ¼å¼")

        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
            raise DatabaseMigrationError(f"å¤‡ä»½å¤±è´¥: {str(e)}")

    async def create_target_tables(self):
        """åœ¨ç›®æ ‡æ•°æ®åº“åˆ›å»ºè¡¨ç»“æ„"""
        try:
            self._report_progress("åˆ›å»ºè¡¨", 15, "æ­£åœ¨åˆ›å»ºPostgreSQLè¡¨ç»“æ„...")

            # å¯¼å…¥Base
            from database import Base

            # åˆ›å»ºå¼‚æ­¥å¼•æ“
            engine = create_async_engine(self.target_db_url, echo=False)

            # åˆ›å»ºæ‰€æœ‰è¡¨
            async with engine.begin() as conn:
                await conn.run_sync(Base.metadata.create_all)

            await engine.dispose()

            self._report_progress("åˆ›å»ºè¡¨", 20, "è¡¨ç»“æ„åˆ›å»ºå®Œæˆ")
            logger.info("âœ… ç›®æ ‡æ•°æ®åº“è¡¨åˆ›å»ºæˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ åˆ›å»ºè¡¨å¤±è´¥: {e}")
            raise DatabaseMigrationError(f"åˆ›å»ºè¡¨å¤±è´¥: {str(e)}")

    async def migrate_table_data(
        self, table_name: str, source_session, target_session: AsyncSession
    ) -> int:
        """è¿ç§»å•ä¸ªè¡¨çš„æ•°æ®"""
        try:
            model = self.models_map.get(table_name)
            if not model:
                logger.warning(f"è·³è¿‡æœªçŸ¥è¡¨: {table_name}")
                return 0

            # ä»æºæ•°æ®åº“è¯»å–æ‰€æœ‰æ•°æ®
            source_data = source_session.query(model).all()
            count = len(source_data)

            if count == 0:
                logger.info(f"è¡¨ {table_name} æ— æ•°æ®ï¼Œè·³è¿‡")
                return 0

            logger.info(f"å¼€å§‹è¿ç§»è¡¨ {table_name}ï¼Œå…± {count} æ¡è®°å½•")

            # æ‰¹é‡æ’å…¥åˆ°ç›®æ ‡æ•°æ®åº“
            batch_size = 100
            for i in range(0, count, batch_size):
                batch = source_data[i : i + batch_size]

                # è½¬æ¢ä¸ºå­—å…¸
                batch_dicts = []
                for item in batch:
                    # è·å–æ‰€æœ‰åˆ—
                    item_dict = {}
                    for column in model.__table__.columns:
                        value = getattr(item, column.name)
                        item_dict[column.name] = value
                    batch_dicts.append(item_dict)

                # æ‰¹é‡æ’å…¥
                for item_dict in batch_dicts:
                    new_obj = model(**item_dict)
                    target_session.add(new_obj)

                await target_session.flush()

                progress = min(99, int((i + batch_size) / count * 30) + 40)
                logger.debug(f"  å·²è¿ç§» {min(i+batch_size, count)}/{count} æ¡")

            await target_session.commit()
            logger.info(f"âœ… è¡¨ {table_name} è¿ç§»å®Œæˆï¼Œå…± {count} æ¡è®°å½•")
            return count

        except Exception as e:
            await target_session.rollback()
            logger.error(f"âŒ è¡¨ {table_name} è¿ç§»å¤±è´¥: {e}")
            raise DatabaseMigrationError(f"è¡¨ {table_name} è¿ç§»å¤±è´¥: {str(e)}")

    async def migrate_all_data(self):
        """è¿ç§»æ‰€æœ‰è¡¨æ•°æ®"""
        try:
            self._report_progress("è¿ç§»æ•°æ®", 25, "å¼€å§‹è¿ç§»æ•°æ®...")

            # åˆ›å»ºæºæ•°æ®åº“è¿æ¥ï¼ˆåŒæ­¥ï¼‰
            source_engine = create_engine(
                self.source_db_url.replace("aiosqlite", "sqlite")
            )
            SourceSession = sessionmaker(bind=source_engine)
            source_session = SourceSession()

            # åˆ›å»ºç›®æ ‡æ•°æ®åº“è¿æ¥ï¼ˆå¼‚æ­¥ï¼‰
            target_engine = create_async_engine(self.target_db_url, echo=False)
            TargetSessionLocal = async_sessionmaker(
                target_engine, class_=AsyncSession, expire_on_commit=False
            )

            total_migrated = 0

            async with TargetSessionLocal() as target_session:
                for idx, table_name in enumerate(self.table_order):
                    progress = 25 + int((idx / len(self.table_order)) * 45)
                    self._report_progress(
                        "è¿ç§»æ•°æ®", progress, f"æ­£åœ¨è¿ç§»è¡¨: {table_name}..."
                    )

                    count = await self.migrate_table_data(
                        table_name, source_session, target_session
                    )
                    total_migrated += count

            # å…³é—­è¿æ¥
            source_session.close()
            source_engine.dispose()
            await target_engine.dispose()

            self._report_progress(
                "è¿ç§»æ•°æ®", 70, f"æ•°æ®è¿ç§»å®Œæˆï¼Œå…± {total_migrated} æ¡è®°å½•"
            )
            logger.info(f"âœ… æ‰€æœ‰æ•°æ®è¿ç§»å®Œæˆï¼Œå…± {total_migrated} æ¡è®°å½•")

            return total_migrated

        except Exception as e:
            logger.error(f"âŒ æ•°æ®è¿ç§»å¤±è´¥: {e}")
            raise DatabaseMigrationError(f"æ•°æ®è¿ç§»å¤±è´¥: {str(e)}")

    async def verify_migration(self) -> Dict[str, Any]:
        """éªŒè¯è¿ç§»ç»“æœ"""
        try:
            self._report_progress("éªŒè¯", 75, "æ­£åœ¨éªŒè¯è¿ç§»ç»“æœ...")

            # è¿æ¥ä¸¤ä¸ªæ•°æ®åº“
            source_engine = create_engine(
                self.source_db_url.replace("aiosqlite", "sqlite")
            )
            target_engine = create_async_engine(self.target_db_url, echo=False)

            verification_results = {}
            all_match = True

            async with target_engine.connect() as target_conn:
                with source_engine.connect() as source_conn:
                    for table_name in self.table_order:
                        # æºæ•°æ®åº“è®¡æ•°
                        source_count = source_conn.execute(
                            text(f"SELECT COUNT(*) FROM {table_name}")
                        ).scalar()

                        # ç›®æ ‡æ•°æ®åº“è®¡æ•°
                        target_count = await target_conn.execute(
                            text(f"SELECT COUNT(*) FROM {table_name}")
                        )
                        target_count = target_count.scalar()

                        match = source_count == target_count
                        verification_results[table_name] = {
                            "source_count": source_count,
                            "target_count": target_count,
                            "match": match,
                        }

                        if not match:
                            all_match = False
                            logger.warning(
                                f"âš ï¸ è¡¨ {table_name} è®°å½•æ•°ä¸åŒ¹é…: "
                                f"æº={source_count}, ç›®æ ‡={target_count}"
                            )
                        else:
                            logger.info(
                                f"âœ… è¡¨ {table_name} éªŒè¯é€šè¿‡: {source_count} æ¡è®°å½•"
                            )

            source_engine.dispose()
            await target_engine.dispose()

            if all_match:
                self._report_progress("éªŒè¯", 85, "âœ… æ‰€æœ‰è¡¨éªŒè¯é€šè¿‡")
                logger.info("âœ… è¿ç§»éªŒè¯æˆåŠŸï¼Œæ‰€æœ‰æ•°æ®ä¸€è‡´")
            else:
                self._report_progress("éªŒè¯", 85, "âš ï¸ éƒ¨åˆ†è¡¨è®°å½•æ•°ä¸åŒ¹é…")
                logger.warning("âš ï¸ è¿ç§»éªŒè¯å‘ç°ä¸ä¸€è‡´")

            return {"all_match": all_match, "details": verification_results}

        except Exception as e:
            logger.error(f"âŒ éªŒè¯å¤±è´¥: {e}")
            raise DatabaseMigrationError(f"éªŒè¯å¤±è´¥: {str(e)}")

    async def update_env_file(
        self, env_file_path: str, postgres_url: str, redis_url: str
    ):
        """æ›´æ–°.envé…ç½®æ–‡ä»¶"""
        try:
            self._report_progress("æ›´æ–°é…ç½®", 90, "æ­£åœ¨æ›´æ–°.envé…ç½®æ–‡ä»¶...")

            env_path = Path(env_file_path)

            # è¯»å–ç°æœ‰é…ç½®
            if env_path.exists():
                with open(env_path, "r", encoding="utf-8") as f:
                    lines = f.readlines()
            else:
                lines = []

            # æ›´æ–°é…ç½®é¡¹
            updates = {
                "DEPLOY_MODE": "standard",
                "DATABASE_URL": postgres_url,
                "CELERY_BROKER_URL": redis_url,
                "CELERY_RESULT_BACKEND": redis_url,
            }

            new_lines = []
            updated_keys = set()

            for line in lines:
                updated = False
                for key, value in updates.items():
                    if line.startswith(f"{key}=") or line.startswith(f"#{key}="):
                        new_lines.append(f"{key}={value}\n")
                        updated_keys.add(key)
                        updated = True
                        break
                if not updated:
                    new_lines.append(line)

            # æ·»åŠ æœªæ‰¾åˆ°çš„é…ç½®é¡¹
            for key, value in updates.items():
                if key not in updated_keys:
                    new_lines.append(f"\n# è‡ªåŠ¨è¿ç§»æ·»åŠ \n{key}={value}\n")

            # å†™å›æ–‡ä»¶
            with open(env_path, "w", encoding="utf-8") as f:
                f.writelines(new_lines)

            self._report_progress("æ›´æ–°é…ç½®", 95, "é…ç½®æ–‡ä»¶æ›´æ–°å®Œæˆ")
            logger.info(f"âœ… .envé…ç½®æ–‡ä»¶å·²æ›´æ–°: {env_path}")

        except Exception as e:
            logger.error(f"âŒ æ›´æ–°é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise DatabaseMigrationError(f"æ›´æ–°é…ç½®æ–‡ä»¶å¤±è´¥: {str(e)}")

    async def run_full_migration(
        self, backup_path: str, env_file_path: str, redis_url: str
    ) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´è¿ç§»æµç¨‹"""
        migration_result = {
            "success": False,
            "backup_file": None,
            "migrated_records": 0,
            "verification": None,
            "error": None,
        }

        try:
            logger.info("=" * 60)
            logger.info("å¼€å§‹æ•°æ®åº“è¿ç§»: SQLite -> PostgreSQL")
            logger.info("=" * 60)

            # 1. æµ‹è¯•ç›®æ ‡æ•°æ®åº“è¿æ¥
            self._report_progress("å‡†å¤‡", 0, "æµ‹è¯•ç›®æ ‡æ•°æ®åº“è¿æ¥...")
            await self.test_target_connection()

            # 2. å¤‡ä»½æºæ•°æ®åº“
            backup_file = await self.backup_source_database(backup_path)
            migration_result["backup_file"] = backup_file

            # 3. åˆ›å»ºç›®æ ‡è¡¨
            await self.create_target_tables()

            # 4. è¿ç§»æ•°æ®
            migrated_count = await self.migrate_all_data()
            migration_result["migrated_records"] = migrated_count

            # 5. éªŒè¯è¿ç§»
            verification = await self.verify_migration()
            migration_result["verification"] = verification

            # 6. æ›´æ–°.envæ–‡ä»¶
            await self.update_env_file(env_file_path, self.target_db_url, redis_url)

            # å®Œæˆ
            self._report_progress("å®Œæˆ", 100, "âœ… è¿ç§»å®Œæˆï¼è¯·é‡å¯æœåŠ¡ä½¿é…ç½®ç”Ÿæ•ˆ")
            logger.info("=" * 60)
            logger.info("âœ… æ•°æ®åº“è¿ç§»æˆåŠŸå®Œæˆï¼")
            logger.info(f"ğŸ“¦ å¤‡ä»½æ–‡ä»¶: {backup_file}")
            logger.info(f"ğŸ“Š è¿ç§»è®°å½•: {migrated_count} æ¡")
            logger.info("ğŸ”„ è¯·é‡å¯æœåŠ¡ä»¥ä½¿ç”¨æ–°æ•°æ®åº“")
            logger.info("=" * 60)

            migration_result["success"] = True
            return migration_result

        except Exception as e:
            migration_result["error"] = str(e)
            logger.error("=" * 60)
            logger.error(f"âŒ è¿ç§»å¤±è´¥: {e}")
            logger.error("=" * 60)
            raise


# æµ‹è¯•å‡½æ•°
async def test_migration():
    """æµ‹è¯•è¿ç§»åŠŸèƒ½"""
    source_url = "sqlite+aiosqlite:///./data/chaoxing.db"
    target_url = (
        "postgresql+asyncpg://chaoxing_user:password@localhost:5432/chaoxing_db"
    )

    def progress_callback(step, progress, message):
        print(f"[{progress}%] {step}: {message}")

    migrator = DatabaseMigrator(source_url, target_url, progress_callback)

    result = await migrator.run_full_migration(
        backup_path="./backups",
        env_file_path="./.env",
        redis_url="redis://localhost:6379/0",
    )

    print("\nè¿ç§»ç»“æœ:")
    print(json.dumps(result, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    asyncio.run(test_migration())
